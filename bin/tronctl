#!/usr/bin/env python
"""Tron Control

Part of the command line interface to the tron daemon. Provides the interface
to controlling jobs and runs.
"""
import argparse
import asyncio
import datetime
import logging
import pprint
import sys
from collections import defaultdict
from typing import Any
from typing import Callable
from typing import Dict
from typing import Generator
from typing import Optional
from typing import Tuple
from urllib.parse import urljoin

import argcomplete  # type: ignore

from tron import __version__
from tron.commands import client
from tron.commands import cmd_utils
from tron.commands.backfill import BackfillRun
from tron.commands.backfill import confirm_backfill
from tron.commands.backfill import DEFAULT_MAX_PARALLEL_RUNS
from tron.commands.backfill import get_date_range
from tron.commands.backfill import LIMIT_MAX_PARALLEL_RUNS
from tron.commands.backfill import print_backfill_cmds
from tron.commands.backfill import print_backfill_runs_table
from tron.commands.backfill import run_backfill_for_date_range
from tron.commands.client import RequestError
from tron.commands.cmd_utils import ExitCode
from tron.commands.cmd_utils import suggest_possibilities
from tron.commands.cmd_utils import tron_jobs_completer
from tron.commands.retry import parse_deps_timeout
from tron.commands.retry import print_retries_table
from tron.commands.retry import retry_actions
from tron.commands.retry import RetryAction

COMMAND_HELP = (
    (
        "start",
        "job name, job run id, or action id",
        "Start the selected job, job run, or action. Creates a new job run if starting a job.",
    ),
    (
        "rerun",
        "job run id",
        "Start a new job run with the same start time command context as the given job run.",
    ),
    (
        "retry",
        "action id",
        "Re-run a job action within an existing job run. Uses latest code/config except the command by default. Add --use-latest-command to use the latest command.",
    ),
    ("recover", "action id", "Ask Tron to start tracking an UNKNOWN action run again"),
    ("cancel", "job run id", "Cancel the selected job run."),
    (
        "backfill",
        "job name",
        "Start job runs for a particular date range",
    ),
    ("disable", "job name", "Disable selected job and cancel any outstanding runs"),
    ("enable", "job name", "Enable the selected job and schedule the next run"),
    (
        "fail",
        "job run or action id",
        "Mark an UNKNOWN job or action as failed. Does not publish action triggers.",
    ),
    (
        "success",
        "job run or action id",
        "Mark an UNKNOWN job or action as having succeeded. Will publish action triggers.",
    ),
    (
        "skip",
        "action id",
        "Skip a failed action, unblocks dependent actions. Does *not* publish action triggers.",
    ),
    (
        "skip-and-publish",
        "action id",
        "Skip a failed action, unblocks dependent actions. *Does* publish action triggers.",
    ),
    ("stop", "action id", "Stop the action run (SIGTERM)"),
    ("kill", "action id", "Force kill the action run (SIGKILL)"),
    ("move", "job name", "Rename a job"),
    ("publish", "action id", "Publish actionrun trigger to kick off downstream jobs"),
    ("discard", "trigger id", "Discard existing actionrun trigger"),
    ("version", None, "Print tron client and server versions"),
)

log = logging.getLogger("tronctl")


def parse_date(date_string):
    return datetime.datetime.strptime(date_string, "%Y-%m-%d")


def parse_cli():
    parser = cmd_utils.build_option_parser()
    subparsers = parser.add_subparsers(dest="command", title="commands", help="Tronctl command to run", required=True)

    cmd_parsers = {}
    for cmd_name, id_help_text, desc in COMMAND_HELP:
        cmd_parsers[cmd_name] = subparsers.add_parser(cmd_name, help=desc, description=desc)
        if id_help_text:
            cmd_parsers[cmd_name].add_argument(
                "id", nargs="*", help=id_help_text
            ).completer = cmd_utils.tron_jobs_completer

    # start
    cmd_parsers["start"].add_argument(
        "--run-date",
        type=parse_date,
        dest="run_date",
        help="What the run-date should be set to",
    )

    # backfill
    backfill_parser = cmd_parsers["backfill"]
    mutex_dates_group = backfill_parser.add_mutually_exclusive_group(required=True)
    mutex_dates_group.add_argument(
        "--start-date",
        type=parse_date,
        dest="start_date",
        help="First run-date to backfill",
    )
    backfill_parser.add_argument(
        "--end-date",
        type=parse_date,
        dest="end_date",
        help=(
            "Last run-date to backfill (note: many jobs operate on date-1), "
            "assuming --start-date is set. This date is inclusive. Defaults to today."
        ),
    )
    backfill_parser.add_argument(
        "--descending",
        action="store_true",
        default=False,
        help=(
            "If set, backfill from end date to start date. Otherwise, "
            "the default is to backfill from start date to end date."
        ),
    )
    mutex_dates_group.add_argument(
        "-d",
        "--dates",
        type=lambda v: [parse_date(date_str.strip()) for date_str in v.split(",")],
        dest="dates",
        help=(
            "List of comma-separated dates to run backfills on. "
            "Backfills will be executed for dates in the order they are presented."
        ),
    )
    backfill_parser.add_argument(
        "-P",
        "--max-parallel",
        type=int,
        dest="max_parallel",
        default=DEFAULT_MAX_PARALLEL_RUNS,
        help=(
            "The max number of dates that can be backfilled in parallel. "
            "Before setting, consider how much in resources your job needs. "
            "If it needs a lot, keep this number low, because there may not be "
            "enough resources in the cluster too satisfy the demand, which can "
            "adversely affect other jobs. "
            "The default is %(default)s."
        ),
    )
    backfill_parser.add_argument(
        "--fail-on-error",
        dest="fail_on_error",
        action="store_true",
        default=False,
        help=(
            "If set, the overall backfill will fail immediately if a backfill "
            "for a single date fails. All in-progress backfills will cancelled. "
            "If a single backfill is still considered successful it was otherwise "
            "cancelled or skipped by the user. "
            "By default, individual backfill failures are ignored."
        ),
    )
    backfill_parser.add_argument(
        "--dry-run",
        action="store_true",
        default=False,
        help="Prints the equivalent `tronctl start` commands for the backfill",
    )

    # retry
    retry_parser = cmd_parsers["retry"]
    retry_parser.add_argument(
        "--use-latest-command",
        action="store_true",
        default=False,
        help="Use the latest command in tronfig rather than the original command when the action run was created",
    )
    retry_parser.add_argument(
        "--wait-for-deps",
        type=parse_deps_timeout,
        default=0,
        dest="deps_timeout",
        help=(
            "Max duration to wait for upstream dependencies (upstream triggers "
            "and/or same job actions) before attempting to retry. "
            "If all dependencies are not done when the timeout expires, "
            "this command will exit with an error, and the action will NOT be retried. "
            "Must be either an int number of seconds, a human-readable/"
            "pytimeparse-parsable string, or 'infinity' to wait forever. "
            "Defaults to 0 (don't wait)."
        ),
    )

    argcomplete.autocomplete(parser)
    args = parser.parse_args()

    return args


def request(url: str, data: Dict[str, Any], headers=None, method=None) -> bool:
    # We want every tronctl request to be attributable
    response = client.request(url, data=data, headers=headers, method=method, user_attribution=True)
    if response.error:
        print(f"Error: {response.content}")
        return False
    print(response.content.get("result", "OK"))
    return True


def event_publish(args):
    for event in args.id:
        yield request(
            urljoin(args.server, "/api/events"),
            dict(command="publish", event=event),
        )


def event_discard(args):
    for event in args.id:
        yield request(
            urljoin(args.server, "/api/events"),
            dict(command="discard", event=event),
        )


def _get_triggers_for_action(server: str, action_identifier: str) -> Optional[Tuple[str, ...]]:
    try:
        namespace, job_name, run_number, action_name = action_identifier.split(".")
    except ValueError:
        print(
            f"Unable to fully decompose {action_identifier}: expected an identifier of the form (namespace).(job).(run).(action)"
        )
        return None

    trigger_response = client.request(
        uri=urljoin(
            server,
            f"/api/jobs/{namespace}.{job_name}/{run_number}/{action_name}",
        ),
    )
    if trigger_response.error:
        print(f"Unable to fetch downstream triggers for {action_identifier}: {trigger_response.error}")
        return None

    # triggers are returned by the API as comma-separated values with a space after every comma, which is
    # not automation-friendly - thus the non-standard multi-character split
    triggers = trigger_response.content.get("trigger_downstreams", "").split(", ")

    # the API will return an empty string for actions with no triggers to emit, but splitting '' yields [''],
    # so we want to make sure that we return an empty iterable in this case
    return tuple(f"{namespace}.{job_name}.{action_name}.{trigger}" for trigger in triggers if trigger)


def control_objects(args: argparse.Namespace):
    tron_client = client.Client(args.server, user_attribution=True)
    url_index = tron_client.index()
    for identifier in args.id:
        try:
            tron_id = client.get_object_type_from_identifier(
                url_index,
                identifier,
            )
        except ValueError as e:
            possibilities = list(
                tron_jobs_completer(prefix="", client=tron_client),
            )
            suggestions = suggest_possibilities(
                word=identifier,
                possibilities=possibilities,
            )
            raise SystemExit(f"Error: {e}{suggestions}")

        if args.command == "skip-and-publish":
            # this command is more of a pseudo-command - skip and publish are handled in two different resources
            # and changing the API would be painful, so instead we call skip + publish separately from the client
            # (i.e., this file) to implement this functionality
            if request(
                url=urljoin(args.server, tron_id.url),
                data={"command": "skip"},
            ):
                # a single action can have 0..N triggers to publish and these can be arbitrarily named, so we need to
                # query the API and figure out what triggers exist
                triggers = _get_triggers_for_action(server=args.server, action_identifier=identifier)
                if triggers is None:
                    print("Encountered error getting triggers to publish.")
                    yield False
                elif not triggers:
                    print(f"{identifier} has no triggers to publish.")
                    # TODO: should we check this up-front and refuse to skip if there are no triggers that will be
                    # published rather than carry on under the assumption that the user copy-pasted/typo'd the identifier?
                    yield True
                else:
                    # TODO: this loop should use event_publish(), but we'd need to refactor how the CLI works and stop passing
                    # around the full set of args everywhere to do so
                    for trigger in triggers:
                        yield request(
                            url=urljoin(args.server, "/api/events"),
                            data={"command": "publish", "event": trigger},
                        )
            else:
                print(f"Unable to skip {identifier}.")
                yield False

        else:
            data = dict(command=args.command)
            if args.command == "start" and args.run_date:
                data["run_time"] = str(args.run_date)
            yield request(urljoin(args.server, tron_id.url), data)


def retry(args):
    if args.deps_timeout != RetryAction.NO_TIMEOUT:
        deps_timeout_str = "forever"  # timeout = -1 (RetryAction.WAIT_FOREVER)
        if args.deps_timeout > 0:
            deps_timeout_str = "up to " + str(datetime.timedelta(seconds=args.deps_timeout))
        print(
            f"We will wait {deps_timeout_str} for all upstream triggers to be published "
            "and required actions to finish successfully before issuing retries for the "
            "following actions:"
        )
        print()
        pprint.pprint(args.id)
        print()

    retries = retry_actions(args.server, args.id, args.use_latest_command, args.deps_timeout)
    print_retries_table(retries)
    yield all([r.succeeded for r in retries])


def move(args):
    try:
        old_name = args.id[0]
        new_name = args.id[1]
    except IndexError as e:
        raise SystemExit(f"Error: Move command needs two arguments.\n{e}")

    tron_client = client.Client(args.server, user_attribution=True)
    url_index = tron_client.index()
    job_index = url_index["jobs"]
    if old_name not in job_index.keys():
        raise SystemExit(f"Error: {old_name} doesn't exist")
    if new_name in job_index.keys():
        raise SystemExit(f"Error: {new_name} exists already")

    data = dict(command="move", old_name=old_name, new_name=new_name)
    yield request(urljoin(args.server, "/api/jobs"), data)


def backfill(args):
    if not args.id:
        print("Error: must provide at least one id argument")
        yield False
    if args.max_parallel > LIMIT_MAX_PARALLEL_RUNS:
        raise SystemExit(
            f"The flag --max-parallel exceeds the allowed limit of {LIMIT_MAX_PARALLEL_RUNS}. "
            + "Please reach out to the Tron team if you need to run backfills with higher limits."
        )

    if args.start_date:
        if args.end_date is None:
            args.end_date = datetime.datetime.today()
        dates = get_date_range(args.start_date, args.end_date, descending=args.descending)
    else:
        dates = args.dates
    date_strs = [d.date().isoformat() for d in dates]

    job_name = args.id[0]
    if args.dry_run:
        print_backfill_cmds(job_name, date_strs)
        yield True
    else:
        if confirm_backfill(job_name, date_strs):
            loop = asyncio.get_event_loop()
            try:
                backfill_runs = loop.run_until_complete(
                    run_backfill_for_date_range(
                        args.server,
                        job_name,
                        dates,
                        max_parallel=args.max_parallel,
                        ignore_errors=(not args.fail_on_error),
                    ),
                )
            finally:
                loop.close()

            print_backfill_runs_table(backfill_runs)
            yield all(br.run_state in BackfillRun.SUCCESS_STATES for br in backfill_runs)


def tron_version(args):
    local_version = __version__
    print(f"Tron client version: {local_version}")
    response = client.request(urljoin(args.server, "/api/status"))
    if response.error:
        print(f"Error: {response.content}")
        yield
    server_version = response.content.get("version", "unknown")
    print(f"Tron server version: {server_version}")
    if server_version != local_version:
        print("Warning: client and server versions should match")
        yield
    yield True


COMMANDS: Dict[str, Callable[[argparse.Namespace], Generator[bool, None, None]]] = defaultdict(
    lambda: control_objects,
    publish=event_publish,
    discard=event_discard,
    backfill=backfill,
    move=move,
    retry=retry,
    version=tron_version,
)


def main():
    """run tronctl"""
    args = parse_cli()
    cmd_utils.setup_logging(args)
    cmd_utils.load_config(args)
    cmd = COMMANDS[args.command]
    try:
        for ret in cmd(args):
            if not ret:
                sys.exit(ExitCode.fail)
    except RequestError as err:
        print(
            f"Error connecting to the tron server ({args.server}): {err}",
            file=sys.stderr,
        )
        sys.exit(ExitCode.fail)


if __name__ == "__main__":
    main()
